from flask import Flask, request, jsonify, send_file
from flask_cors import CORS
import time
import pandas as pd
import os
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup

import re

def extract_review_text(raw_html):
    """
    HTMLì—ì„œ ë¦¬ë·° ë³¸ë¬¸ë§Œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜ (ì´ë¯¸ì§€ íƒœê·¸ ì œê±° í¬í•¨)
    """
    # âœ… <span> íƒœê·¸ ì¤‘ <img>ë¥¼ í¬í•¨í•œ íƒœê·¸ ì œê±°
    raw_html = re.sub(r'<span[^>]*><img.*?></span>', '', raw_html, flags=re.DOTALL)

    # âœ… <span> íƒœê·¸ ì•ˆì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    review_texts = re.findall(r'<span[^>]*>(.*?)</span>', raw_html, re.DOTALL)

    # âœ… ë¶ˆí•„ìš”í•œ í‚¤ì›Œë“œ ì œê±°
    exclude_keywords = ["í‰ì ", "ì‹ ê³ ", "ì´ë¯¸ì§€ í¼ì³ë³´ê¸°", "ë”ë³´ê¸°", "ë¦¬ë·° ë”ë³´ê¸°/ì ‘ê¸°", "ì‚¬ì§„/ë¹„ë””ì˜¤ ìˆ˜", "ìŠ¤í† ì–´PICK", "í•œë‹¬ì‚¬ìš©"]

    clean_texts = []
    for text in review_texts:
        text = text.strip()
        if not any(keyword in text for keyword in exclude_keywords):
            clean_texts.append(text)

    # âœ… ì •ì œëœ í…ìŠ¤íŠ¸ ë°˜í™˜ (ì—¬ëŸ¬ ì¤„ì„ í•˜ë‚˜ë¡œ í•©ì¹¨)
    return " ".join(clean_texts)

app = Flask(__name__)
CORS(app)  # CORS í—ˆìš©

# í¬ë¡¬ ë“œë¼ì´ë²„ ì„¤ì •
options = webdriver.ChromeOptions()
options.add_argument("--start-maximized")  # ë¸Œë¼ìš°ì € ìµœëŒ€í™”
driver = None  # ì „ì—­ ë³€ìˆ˜

@app.route('/start', methods=['GET'])
def start_browser():
    """ ë¸Œë¼ìš°ì € ì‹¤í–‰ (ë¡œê·¸ì¸ìš©) """
    global driver
    if driver is None:
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)

    driver.get("https://www.jobplanet.co.kr/users/sign_in")
    return jsonify({"message": "ë¸Œë¼ìš°ì € ì‹¤í–‰ ì™„ë£Œ. ë¡œê·¸ì¸ í›„ 'í¬ë¡¤ë§ ì‹œì‘' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”."})

@app.route('/crawl', methods=['POST'])
def crawl():
    """ ë¡œê·¸ì¸ í›„ í¬ë¡¤ë§ ì‹œì‘ """
    global driver
    if driver is None:
        return jsonify({"message": "ë¨¼ì € ë¸Œë¼ìš°ì €ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."})

    data = request.json
    target_url = data.get("target_url")

    if not target_url:
        return jsonify({"message": "í¬ë¡¤ë§í•  URLì´ í•„ìš”í•©ë‹ˆë‹¤."})

    parsed_data = []
    page = 1

    while True:
        url = f"{target_url}?page={page}"
        driver.get(url)
        time.sleep(2)

        soup = BeautifulSoup(driver.page_source, 'html.parser')
        content_divs = soup.find_all('div', class_='content_body_ty1')

        if len(content_divs) < 1:
            break

        new_data = parse_content(content_divs)
        parsed_data.extend(new_data)
        print(f"Page {page} parsed and added to data.")
        page += 1

    # ì—‘ì…€ë¡œ ì €ì¥
    output_file = "merged_review.xlsx"
    columns = ["ë²ˆí˜¸", "ì¸í„°ë·° ë‚´ìš©", "ë©´ì ‘ ì§ˆë¬¸", "ë©´ì ‘ ë‹µë³€", "ì±„ìš© ë°©ì‹", "ë°œí‘œ ì‹œê¸°"]  # 6ê°œë¡œ ìˆ˜ì •

    output_df = pd.DataFrame(parsed_data, columns=columns)
    output_df.to_excel(output_file, index=False)
    print(f"Data saved to {output_file}")

    # 'ì¸í„°ë·° ë‚´ìš©'ê³¼ 'ë©´ì ‘ ë‹µë³€' ì»¬ëŸ¼ì„ í•©ì³ ìƒˆë¡œìš´ ì»¬ëŸ¼ ìƒì„±
    data = pd.read_excel(output_file)
    data['review'] = 'ì‘ë‹µìì˜ í•œì¤„í‰ : ' + data['ì¸í„°ë·° ë‚´ìš©'].astype(str) + '\nì‘ë‹µìì˜ ë©´ì ‘ëŠë‚€ì  : ' + data['ë©´ì ‘ ë‹µë³€'].astype(str)

    # ê²°ê³¼ë¥¼ ìƒˆë¡œìš´ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥
    data.to_excel(output_file, index=False)
    print(f"Updated data saved to {output_file}")

    # âœ… íŒŒì¼ ë‹¤ìš´ë¡œë“œ URLì„ í´ë¼ì´ì–¸íŠ¸ì— ë°˜í™˜
    return jsonify({"message": "í¬ë¡¤ë§ ì™„ë£Œ", "download_url": f"http://localhost:5000/download/{output_file}"})


@app.route('/download/<filename>', methods=['GET'])
def download_file(filename):
    """ í´ë¼ì´ì–¸íŠ¸ì—ì„œ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆë„ë¡ ì œê³µ """
    file_path = os.path.join(os.getcwd(), filename)
    if os.path.exists(file_path):
        return send_file(file_path, as_attachment=True)
    else:
        return jsonify({"message": "íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}), 404

def parse_content(divs):
    """ BeautifulSoupìœ¼ë¡œ í˜ì´ì§€ ë‚´ìš© íŒŒì‹± """
    parsed_data = []
    for idx, div in enumerate(divs, start=1):
        try:
            interview_text = div.find('div', class_='us_label_wrap').find('h2', class_='us_label').get_text(strip=True)
        except:
            interview_text = ''

        try:
            tc_list = div.find('dl', class_='tc_list')
            dd_tags = tc_list.find_all('dd', class_='df1')
            interview_question = dd_tags[0].get_text(separator=" ", strip=True) if len(dd_tags) > 0 else ''
            interview_answer = dd_tags[1].get_text(separator=" ", strip=True) if len(dd_tags) > 1 else ''
            recruitment_method = dd_tags[2].get_text(separator=" ", strip=True) if len(dd_tags) > 2 else ''
            announcement_timing = dd_tags[3].get_text(separator=" ", strip=True) if len(dd_tags) > 3 else ''
        except:
            interview_question = interview_answer = recruitment_method = announcement_timing = ''

        parsed_data.append([idx, interview_text, interview_question, interview_answer,
                            recruitment_method, announcement_timing])
    return parsed_data
@app.route('/start-naver', methods=['GET'])
def start_naver_browser():
    """ ë„¤ì´ë²„ ì‡¼í•‘ëª° ë¸Œë¼ìš°ì € ì‹¤í–‰ """
    global driver
    if driver is None:
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)

    driver.get("https://shopping.naver.com/home")  # ë„¤ì´ë²„ ì‡¼í•‘ í™ˆìœ¼ë¡œ ì´ë™
    return jsonify({"message": "ë¸Œë¼ìš°ì €ê°€ ì‹¤í–‰ë˜ì—ˆìŠµë‹ˆë‹¤. ì›í•˜ëŠ” ìƒí’ˆ í˜ì´ì§€ë¡œ ì´ë™ í›„ 'í¬ë¡¤ë§ ì‹œì‘' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”."})

@app.route('/naver-crawl', methods=['POST'])
def naver_crawl():
    """ ë„¤ì´ë²„ ì‡¼í•‘ ë¦¬ë·° í¬ë¡¤ë§ ì‹œì‘ """
    global driver
    if driver is None:
        return jsonify({"message": "ë¨¼ì € ë¸Œë¼ìš°ì €ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."})

    data = request.json
    max_pages = int(data.get("max_pages", 10))  # ê¸°ë³¸ê°’ 10í˜ì´ì§€

    parsed_data = []
    current_page = 1  

    while current_page <= max_pages:
        print(f"ğŸ” {current_page} í˜ì´ì§€ í¬ë¡¤ë§ ì¤‘...")

        try:
            review_elements = WebDriverWait(driver, 10).until(
                EC.presence_of_all_elements_located((By.XPATH, "//li[contains(@class, 'BnwL_cs1av')]"))
            )
        except:
            print("ğŸš« ë¦¬ë·° ë¡œë”© ì‹¤íŒ¨")
            break

        for review in review_elements:
            try:
                username = review.find_element(By.XPATH, ".//strong[contains(@class, '_2L3vDiadT9')]").text
                date = review.find_element(By.XPATH, ".//span[contains(@class, '_2L3vDiadT9')]").text
                rating = review.find_element(By.XPATH, ".//em[contains(@class, '_15NU42F3kT')]").text
                
                raw_html = review.get_attribute('innerHTML')
                review_content = extract_review_text(raw_html)

                parsed_data.append([username, date, rating, review_content])
                print(f"âœ… {username} | {date} | í‰ì : {rating}\n{review_content}\n\n" + "-" * 60)
            except Exception as e:
                print(f"âŒ ë¦¬ë·° ì¶”ì¶œ ì˜¤ë¥˜: {e}")

        # âœ… í˜ì´ì§€ë„¤ì´ì…˜ ë¡œì§
        if current_page % 10 != 0:
            try:
                next_page_button = driver.find_element(By.XPATH, f"//a[text()='{current_page + 1}']")
                driver.execute_script("arguments[0].click();", next_page_button)
                time.sleep(3)
                current_page += 1
                continue
            except:
                print(f"ğŸš« {current_page+1} í˜ì´ì§€ ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ.")
                break
        else:
            try:
                next_button = driver.find_element(By.XPATH, "//a[contains(text(), 'ë‹¤ìŒ')]")
                driver.execute_script("arguments[0].click();", next_button)
                time.sleep(3)
                current_page += 1
            except:
                print("ğŸš« 'ë‹¤ìŒ' ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ. í¬ë¡¤ë§ ì¢…ë£Œ.")
                break

    # âœ… ì—‘ì…€ íŒŒì¼ ì €ì¥
    output_file = "naver_reviews.xlsx"
    columns = ["ì‘ì„±ì", "ë‚ ì§œ", "í‰ì ", "ë¦¬ë·°"]
    output_df = pd.DataFrame(parsed_data, columns=columns)
    output_df.to_excel(output_file, index=False)
    print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ! ë°ì´í„° ì €ì¥ë¨: `{output_file}`")

    return jsonify({"message": "í¬ë¡¤ë§ ì™„ë£Œ", "download_url": f"http://localhost:5000/download/{output_file}"})
@app.route('/start-naver-price', methods=['GET'])
def start_naver_price_browser():
    """ ë„¤ì´ë²„ ê°€ê²© ë¦¬ë·° í¬ë¡¤ë§ ë¸Œë¼ìš°ì € ì‹¤í–‰ """
    global driver
    if driver is None:
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)

    driver.get("https://shopping.naver.com/home")  # ë„¤ì´ë²„ ì‡¼í•‘ í™ˆìœ¼ë¡œ ì´ë™
    return jsonify({"message": "ë¸Œë¼ìš°ì €ê°€ ì‹¤í–‰ë˜ì—ˆìŠµë‹ˆë‹¤. ì›í•˜ëŠ” ìƒí’ˆ í˜ì´ì§€ë¡œ ì´ë™ í›„ 'í¬ë¡¤ë§ ì‹œì‘' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”."})


@app.route('/naver-price-crawl', methods=['POST'])
def naver_price_crawl():
    """ ë„¤ì´ë²„ ê°€ê²© ë¦¬ë·° í¬ë¡¤ë§ ì‹œì‘ (ì‚¬ìš©ìê°€ ì§ì ‘ í˜ì´ì§€ ì´ë™ í›„ ì‹¤í–‰) """
    global driver
    if driver is None:
        return jsonify({"message": "ë¨¼ì € ë¸Œë¼ìš°ì €ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."})

    data = request.json
    max_pages = int(data.get("max_pages", 10))  # ê¸°ë³¸ 10í˜ì´ì§€

    parsed_data = []
    current_page = 1

    while current_page <= max_pages:
        print(f"ğŸ” {current_page} í˜ì´ì§€ í¬ë¡¤ë§ ì¤‘...")

        try:
            # ê´‘ê³  ì œì™¸í•œ ìƒí’ˆ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
            product_elements = WebDriverWait(driver, 10).until(
                EC.presence_of_all_elements_located((By.XPATH, "//div[contains(@class, 'product_item__MDtDF')]"))
            )
        except:
            print("ğŸš« ìƒí’ˆ ëª©ë¡ ë¡œë”© ì‹¤íŒ¨")
            break

        for product in product_elements:
            try:
                # ê´‘ê³  ì œí’ˆ í•„í„°ë§
                is_ad = (
                    len(product.find_elements(By.XPATH, ".//svg[contains(@class, 'svg_ad')]")) > 0
                    or "superSavingProduct_" in product.get_attribute("class")
                )
                if is_ad:
                    continue
                
                # ì‡¼í•‘ëª°ëª… ë´¤ì„ ë•Œ, ì¿ íŒ¡ì´ë‚˜ ê·¸ëŸ° ì”ë°”ë¦¬ë©´ ì¬ë‚Œë‚Œ
                try:
                    shop_name_element = product.find_element(By.XPATH, ".//a[contains(@class, 'product_mall')]")
                    shop_name = shop_name_element.text.strip()

                    # âœ… ì‡¼í•‘ëª° ì´ë¦„ì´ ë¹ˆ ë¬¸ìì—´ì´ë©´ ì´ë¯¸ì§€ì˜ alt ì†ì„±ì—ì„œ ê°€ì ¸ì˜¤ê¸°
                    if not shop_name:
                        continue
                        
                except:
                    shop_name = "ì •ë³´ ì—†ìŒ"
                # ìƒí’ˆëª…
                product_name = product.find_element(By.XPATH, ".//div[contains(@class, 'product_title')]/a").text
                
                # ê°€ê²©
                price = product.find_element(By.XPATH, ".//span[contains(@class, 'price_num')]").text.replace(",", "")
                
                
                try:
                # âœ… ë¦¬ë·° ìˆ˜ & ì°œ ìˆ˜ë¥¼ í¬í•¨í•˜ëŠ” ì „ì²´ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
                    etc_box = product.find_element(By.CLASS_NAME, "product_etc_box__ElfVA").text

                    # âœ… ë¦¬ë·° ìˆ˜ ì •ê·œì‹ìœ¼ë¡œ ì¶”ì¶œ (ex: "(3ë§Œ)" ë˜ëŠ” "(244)")
                    review_match = re.search(r"\(([\d,]+(?:\.\d+)?[ë§Œ]?)\)", etc_box)
                    if review_match:
                        review_count = review_match.group(1)
                        # "3ë§Œ" ê°™ì€ ê²½ìš° "30000" ìœ¼ë¡œ ë³€í™˜
                        if "ë§Œ" in review_count:
                            review_count = review_count.replace("ë§Œ", "")
                            review_count = str(int(float(review_count) * 10000))
                    else:
                        review_count = "0"  # ë¦¬ë·° ì—†ìŒ

                    # âœ… ì°œ ìˆ˜ ì •ê·œì‹ìœ¼ë¡œ ì¶”ì¶œ (ex: "ì°œ 1,463")
                    jjim_match = re.search(r"ì°œ\s([\d,]+)", etc_box)
                    if jjim_match:
                        jjim_count = jjim_match.group(1).replace(",", "")
                    else:
                        jjim_count = "0"  # ì°œ ì—†ìŒ

                except:
                    review_count = "0"
                    jjim_count = "0"
                # âœ… ìš©ëŸ‰ (ìš°ì„  íƒœê·¸ì—ì„œ ì°¾ê³ , ì—†ìœ¼ë©´ ì œí’ˆëª…ì—ì„œ ì •ê·œì‹ìœ¼ë¡œ ì¶”ì¶œ)
                try:
                    capacity_element = driver.find_element(By.XPATH, "//a[contains(@data-shp-contents-type, 'ìš©ëŸ‰_M')]")
                    capacity = capacity_element.text.replace("ìš©ëŸ‰ : ", "").strip()
                except:
                    # ì œí’ˆëª…ì—ì„œ ì •ê·œì‹ìœ¼ë¡œ ìš©ëŸ‰ ì¶”ì¶œ
                    import re
                    match = re.search(r'(\d+(?:\.\d+)?)(ml|g|l|kg)', product_name.lower().replace(" ", ""))
                    capacity = match.group(0) if match else "ì •ë³´ ì—†ìŒ"

                # êµ¬ë§¤ ìˆ˜
                try:
                    purchase_count = product.find_element(By.XPATH, ".//span[contains(text(), 'êµ¬ë§¤')]/em").text
                except:
                    purchase_count = "0"

                # ë“±ë¡ì¼
                try:
                    reg_date = product.find_element(By.XPATH, ".//span[contains(text(), 'ë“±ë¡ì¼')]").text.replace("ë“±ë¡ì¼ ", "")
                except:
                    reg_date = "ì •ë³´ ì—†ìŒ"

                
                try:
                    delivery_element = product.find_element(By.CLASS_NAME, "price_delivery__yw_We")
                    delivery_text = delivery_element.text.strip()

                    # ìˆ«ì + ì› ë˜ëŠ” "ë¬´ë£Œ" ë§Œ ì¶”ì¶œ
                    delivery_match = re.search(r"(\d{1,3}(,\d{3})*ì›|ë¬´ë£Œ)", delivery_text)
                    delivery_fee = delivery_match.group(1) if delivery_match else "0ì›"

                except:
                    delivery_fee = "0ì›"  # ê¸°ë³¸ê°’ ì„¤ì •


                # âœ… ì¢…ë¥˜, íš¨ê³¼, íŠ¹ì§•, í¬ì¥í˜•íƒœ ì¶”ì¶œ
                try:
                    desc_elements = product.find_elements(By.XPATH, ".//div[contains(@class, 'product_desc__m2mVJ')]/div/a")
                    product_type, effects, features, packaging = "", [], [], []

                    for desc in desc_elements:
                        desc_text = desc.text
                        if "ì¢…ë¥˜ :" in desc_text:
                            product_type = desc_text.replace("ì¢…ë¥˜ :", "").strip()
                        elif "íš¨ê³¼ :" in desc_text or "íš¨ê³¼" in desc_text:
                            effects.append(desc_text.replace("íš¨ê³¼ :", "").strip())
                        elif "íŠ¹ì§• :" in desc_text or "íŠ¹ì§•" in desc_text:
                            features.append(desc_text.replace("íŠ¹ì§• :", "").strip())
                        elif "í¬ì¥í˜•íƒœ :" in desc_text or "í¬ì¥í˜•íƒœ" in desc_text:
                            packaging.append(desc_text.replace("í¬ì¥í˜•íƒœ :", "").strip())

                    effects_text = ", ".join(effects) if effects else "ì •ë³´ ì—†ìŒ"
                    features_text = ", ".join(features) if features else "ì •ë³´ ì—†ìŒ"
                    packaging_text = ", ".join(packaging) if packaging else "ì •ë³´ ì—†ìŒ"

                except:
                    product_type, effects_text, features_text, packaging_text = "ì •ë³´ ì—†ìŒ", "ì •ë³´ ì—†ìŒ", "ì •ë³´ ì—†ìŒ", "ì •ë³´ ì—†ìŒ"

                # âœ… ì¹´í…Œê³ ë¦¬ Depth (ìµœëŒ€ 4ê°œ)
                try:
                    category_elements = product.find_elements(By.XPATH, ".//div[contains(@class, 'product_depth__I4SqY')]/span")
                    categories = [cat.text for cat in category_elements]
                    depth1 = categories[0] if len(categories) > 0 else "ì •ë³´ ì—†ìŒ"
                    depth2 = categories[1] if len(categories) > 1 else "ì •ë³´ ì—†ìŒ"
                    depth3 = categories[2] if len(categories) > 2 else "ì •ë³´ ì—†ìŒ"
                    depth4 = categories[3] if len(categories) > 3 else "ì •ë³´ ì—†ìŒ"
                except:
                    depth1, depth2, depth3, depth4 = "ì •ë³´ ì—†ìŒ", "ì •ë³´ ì—†ìŒ", "ì •ë³´ ì—†ìŒ", "ì •ë³´ ì—†ìŒ"

                # ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                parsed_data.append([product_name, price, capacity,review_count,  purchase_count, reg_date,delivery_fee ,shop_name,
                                    product_type, effects_text, features_text, packaging_text, depth1, depth2, depth3, depth4])

                print(f"âœ… {product_name} | {price}ì› | ë¦¬ë·°: {review_count} | ì°œ: {jjim_count} | êµ¬ë§¤: {purchase_count} | {shop_name}")
                print(f"   ì¹´í…Œê³ ë¦¬: {depth1} > {depth2} > {depth3} > {depth4}")
                print(f"   ì¢…ë¥˜: {product_type} | íš¨ê³¼: {effects_text} | íŠ¹ì§•: {features_text} | í¬ì¥í˜•íƒœ: {packaging_text}")

            except Exception as e:
                print(f"âŒ ìƒí’ˆ ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {e}")

        # âœ… í˜ì´ì§€ë„¤ì´ì…˜ ë¡œì§
        if current_page % 10 != 0:
            try:
                next_page_button = driver.find_element(By.XPATH, f"//a[text()='{current_page + 1}']")
                driver.execute_script("arguments[0].click();", next_page_button)
                time.sleep(3)
                current_page += 1
                continue
            except:
                print(f"ğŸš« {current_page+1} í˜ì´ì§€ ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ.")
                break
        else:
            try:
                next_button = driver.find_element(By.XPATH, "//a[contains(text(), 'ë‹¤ìŒ')]")
                driver.execute_script("arguments[0].click();", next_button)
                time.sleep(3)
                current_page += 1
            except:
                print("ğŸš« 'ë‹¤ìŒ' ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ. í¬ë¡¤ë§ ì¢…ë£Œ.")
                break

    # âœ… ì—‘ì…€ íŒŒì¼ ì €ì¥
    output_file = "naver_price.xlsx"
    columns = ["ìƒí’ˆëª…", "ê°€ê²©","ìš©ëŸ‰" ,"ë¦¬ë·° ìˆ˜",  "êµ¬ë§¤ ìˆ˜", "ë“±ë¡ì¼","ë°°ì†¡ë¹„", "ì‡¼í•‘í•‘ëª°ëª…", "ì¢…ë¥˜", "íš¨ê³¼", "íŠ¹ì§•", "í¬ì¥í˜•íƒœ", "Depth1", "Depth2", "Depth3", "Depth4"]
    output_df = pd.DataFrame(parsed_data, columns=columns)
    output_df.to_excel(output_file, index=False)
    print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ! ë°ì´í„° ì €ì¥ë¨: `{output_file}`")

    return jsonify({"message": "í¬ë¡¤ë§ ì™„ë£Œ", "download_url": f"http://localhost:5000/download/{output_file}"})


if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=True)
